{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The goal of an agent in Reinforcement learning is to maximise some numerical value that represents some objective. In order to do so the agent must learn the optimal policy, which is a function of states to actions to take at those states. One method used to solve these types of problems is Q Learning.\n",
    "\n",
    "Q Learning is a Value iteration method used to solve RL problems formalised as a finite MDP by taking advantage of Value functions and more specifically, the action-value function. The action-value function or Q function determines the Q value of a state action pair"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Install packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gym numpy 'gym[toy_text]' pyglet > /dev/null 2>&1  # > /dev/null 2>&1 sends stdout and stderr to /dev/null (the \"void\") instead of displaying below"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "print(gym.__version__)  # 0.26.1 as of time of writing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = None\n",
    "np.random.seed(seed)\n",
    "# Why it needs to be set for tabular"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Environment\n",
    "\n",
    "- The environment is the world in which the agent lives. It is formalised as a finite Markov Decision Process where the rewards and state transition probabilities are unknown to the agent.\n",
    "- FrozenLake is a grid environment that is described as a frozen over lake containing holes in it in which the agent can fall through. The ice is also slippy so there is a possibility the agent may slip in either adjacent direction of the chosen action (meaning state transitions are stochastic not deterministic).\n",
    "- At every time step the agent has access to four actions, Up, Down, Left and Right. If on the edge of the grid, the agent will not be able to move off the grid and remain in the same state if they try to do so (or adjacent if they slip)\n",
    "- This is a Tabular Stochastic Environment and standard Q Learning is capable of solving it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "env_id = \"FrozenLake-v1\"\n",
    "# env_id = \"FrozenLake8x8-v1\"\n",
    "env = gym.make(env_id, render_mode='ansi')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see the environment in action:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting State \n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 0 after action  (Right)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 1 after action  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 2 after action  (Down)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 3 after action  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 4 after action  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 5 after action  (Right)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 6 after action  (Right)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 7 after action  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "State at step 8 after action  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "State at step 9 after action  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001B[41mH\u001B[0mFFG\n",
      "\n",
      "Terminal state reached after action  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001B[41mH\u001B[0mFFG\n",
      "\n",
      "New Episode\n",
      "Starting State \n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 10 after action  (Right)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 11 after action  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 12 after action  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 13 after action  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 14 after action  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 15 after action  (Down)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Terminal state reached after action  (Down)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "New Episode\n",
      "Starting State \n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 16 after action  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 17 after action  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 18 after action  (Down)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 19 after action  (Right)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 20 after action  (Down)\n",
      "SFFF\n",
      "FH\u001B[41mF\u001B[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 21 after action  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001B[41mF\u001B[0mH\n",
      "HFFG\n",
      "\n",
      "State at step 22 after action  (Up)\n",
      "SFFF\n",
      "FH\u001B[41mF\u001B[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 23 after action  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001B[41mF\u001B[0mH\n",
      "HFFG\n",
      "\n",
      "State at step 24 after action  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001B[41mF\u001B[0mFH\n",
      "HFFG\n",
      "\n",
      "State at step 25 after action  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001B[41mF\u001B[0mFG\n",
      "\n",
      "State at step 26 after action  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001B[41mF\u001B[0mFG\n",
      "\n",
      "State at step 27 after action  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001B[41mF\u001B[0mFH\n",
      "HFFG\n",
      "\n",
      "State at step 28 after action  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001B[41mF\u001B[0mH\n",
      "HFFG\n",
      "\n",
      "State at step 29 after action  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001B[41mF\u001B[0mG\n",
      "\n",
      "State at step 30 after action  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001B[41mF\u001B[0mFG\n",
      "\n",
      "State at step 31 after action  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001B[41mF\u001B[0mG\n",
      "\n",
      "State at step 32 after action  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001B[41mG\u001B[0m\n",
      "\n",
      "Terminal state reached after action  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001B[41mG\u001B[0m\n",
      "\n",
      "New Episode\n",
      "Starting State \n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 33 after action  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 34 after action  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 35 after action  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 36 after action  (Right)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 37 after action  (Down)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 38 after action  (Left)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 39 after action  (Down)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 40 after action  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 41 after action  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 42 after action  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 43 after action  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 44 after action  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 45 after action  (Down)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 46 after action  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 47 after action  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 48 after action  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State at step 49 after action  (Down)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()  # Environment needs to be reset every time a new episode needs to start\n",
    "print(\"Starting State\", env.render())\n",
    "for step in range(50):  # Time steps\n",
    "\n",
    "    rand_action = env.action_space.sample()  # Samples a random action from all possible actions\n",
    "\n",
    "    next_state, reward, terminal, _, prob = env.step(rand_action)  # The step function is the whole environment as a function. Takes in actions and outputs information such as reward, the next state, if it was a terminal state and the state transition probability\n",
    "\n",
    "    print(f\"State at step {step} after action{env.render()}\")  # the render function returns data to see what is happening in the environment\n",
    "\n",
    "    if terminal:  # If we land on a terminal state the episode is over, and we need to reset to start the next episode\n",
    "        print(f\"Terminal state reached after action{env.render()}\")  # the render function returns data to see what is happening in the environment\n",
    "        env.reset()\n",
    "        print(\"New Episode\")\n",
    "        print(\"Starting State\", env.render())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After running the previous cell we can see the agent randomly moving around its environment. However, we can also see strange behaviour of the agent when the agent may choose to move in a certain direction but it did not end up in its desired state. This is due to the environment being stochastic and the agent having a 33% chance of moving in its desired action and another 33% chance of moving in either adjacent direction."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make the Agent\n",
    "- Q Learning -> Find the optimal policy by learning the Q Values for each state-action pair\n",
    "- q_pi(s, a) = E[G_t | S_t = a, A_t = a] = E[sum(gamma^k * R_t+k+1 | S_t = a, A_t = a]\n",
    "\n",
    "## The Agent\n",
    "In Q Learning the agents aim is to find the optimal policy by learning the Q values for each state action pair using the Bellman and then following the Optimal Action-Value Function from there on out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "\n",
    "    def __init__(self, q_env: gym.envs, agent_seed = None):\n",
    "        self.env = q_env  # The environment the agent will live in\n",
    "        self.seed = agent_seed\n",
    "\n",
    "        self.obs_space = self.env.observation_space.n  # Number of states\n",
    "        self.action_space = self.env.action_space.n  # Number of actions\n",
    "        self.q_table = np.zeros((self.obs_space, self.action_space))  # Table of shape (States, Actions)\n",
    "\n",
    "        self.alpha = 0.1  # Learning rate\n",
    "        self.gamma = 0.99  # Expected return discount factor\n",
    "\n",
    "        self.max_time_steps = 0\n",
    "        self.max_epsilon = 1.0\n",
    "        self.min_epsilon = 0.05\n",
    "\n",
    "\n",
    "    def learn(self, max_time_steps):\n",
    "        \"\"\"\n",
    "        Attempt to learn an optimal policy - Q Learning does so be estimating the Q Value of each state action pair\n",
    "        :param max_time_steps:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.max_time_steps = max_time_steps\n",
    "\n",
    "        # Log data\n",
    "        total_reward = 0\n",
    "        total_episodes = 0\n",
    "        episode_steps = 0\n",
    "        episode_wins = 0\n",
    "\n",
    "        rewards = []\n",
    "\n",
    "        state, _ = self.env.reset(seed=self.seed)  # Get initial state\n",
    "        for steps in range(max_time_steps):  # Run training until max steps is reached\n",
    "\n",
    "            epsilon = max(self.min_epsilon, self.max_epsilon - ((self.max_epsilon / max_time_steps) * steps))\n",
    "            if epsilon > np.random.random():\n",
    "                action =  self.env.action_space.sample()\n",
    "            else:\n",
    "                action = self.get_action(state)  # Choose an action based on the current state\n",
    "\n",
    "\n",
    "            next_state, reward, done, _, prob = self.env.step(action)  # Take the action\n",
    "            self._train(state, action, reward, next_state)  # Train the agent\n",
    "\n",
    "            # Update data\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            episode_steps += 1\n",
    "\n",
    "            if done or self.env.spec.max_episode_steps <= episode_steps:  # If a terminal state or max steps is reached\n",
    "                state, _ = self.env.reset(seed=self.seed)  # Get initial state\n",
    "                total_episodes += 1  # Increment number of Episodes completed\n",
    "                rewards.append(total_reward)\n",
    "\n",
    "                print({\n",
    "                    \"Episode\" : total_episodes,\n",
    "                    \"Reward\": total_reward,\n",
    "                    \"Steps\" : episode_steps\n",
    "                })\n",
    "\n",
    "                # Reset episodic data\n",
    "                total_reward = 0\n",
    "                episode_steps = 0\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        \"\"\"\n",
    "        Check the Q Table to see which Action at State \"obs\" returns the greatest Q Value\n",
    "        Greedy function used to determine if action is Explore or Exploit\n",
    "        :param obs: The current state the agent exists at\n",
    "        :return: The action that is expected to return the largest Q Value\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        return np.argmax(self.q_table[obs, :])\n",
    "\n",
    "\n",
    "    def test(self, episodes = 5):\n",
    "        # Show the Agent interacting with its environment\n",
    "        test_rewards = []\n",
    "        test_total_reward = 0\n",
    "\n",
    "        state = self.env.reset()[0]\n",
    "        for episode in range(episodes):\n",
    "            action = self.get_action(state)  # Choose an action based on the current state\n",
    "            next_state, reward, done, _, prob = self.env.step(action)  # Take the action\n",
    "            test_total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                test_rewards.append(test_total_reward)\n",
    "                test_total_reward = 0\n",
    "\n",
    "    def _train(self, s, a, r, ns):\n",
    "        \"\"\"\n",
    "        Value Iteration\n",
    "        Update the Q Value for each state-action pair by using the Bellman optimality equation until convergence\n",
    "        => q* (s, a) = E[R_t+1 + gamma * max(a') q* (s', a')\n",
    "        =>\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        gradient = (1 - self.alpha) + self.alpha * (r + self.gamma * np.max(self.q_table[ns, :]))\n",
    "        self.q_table[s, a] *= gradient\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the Environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent = QAgent(env, agent_seed=seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_steps = 1_000_000\n",
    "agent.learn(max_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.env.render_mode = \"human\"\n",
    "agent.test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}